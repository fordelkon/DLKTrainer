{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70033bec",
   "metadata": {},
   "source": [
    "## Diffusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f093c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964d23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels, size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd64e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return nn.functional.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf35d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21c4ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337bf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, c_in=3, c_out=3, time_dim=256):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_dim\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.sa1 = SelfAttention(128, 32)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa2 = SelfAttention(256, 16)\n",
    "        self.down3 = Down(256, 256)\n",
    "        self.sa3 = SelfAttention(256, 8)\n",
    "\n",
    "        self.bot1 = DoubleConv(256, 512)\n",
    "        self.bot2 = DoubleConv(512, 512)\n",
    "        self.bot3 = DoubleConv(512, 256)\n",
    "\n",
    "        self.up1 = Up(512, 128)\n",
    "        self.sa4 = SelfAttention(128, 16)\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.sa5 = SelfAttention(64, 32)\n",
    "        self.up3 = Up(128, 64)\n",
    "        self.sa6 = SelfAttention(64, 64)\n",
    "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2).float() / channels)\n",
    "        ).to(t.device)\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1, t)\n",
    "        x2 = self.sa1(x2)\n",
    "        x3 = self.down2(x2, t)\n",
    "        x3 = self.sa2(x3)\n",
    "        x4 = self.down3(x3, t)\n",
    "        x4 = self.sa3(x4)\n",
    "\n",
    "        x4 = self.bot1(x4)\n",
    "        x4 = self.bot2(x4)\n",
    "        x4 = self.bot3(x4)\n",
    "\n",
    "        x = self.up1(x4, x3, t)\n",
    "        x = self.sa4(x)\n",
    "        x = self.up2(x, x2, t)\n",
    "        x = self.sa5(x)\n",
    "        x = self.up3(x, x1, t)\n",
    "        x = self.sa6(x)\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "090b71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Custom Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, image_num=-1):\n",
    "        # Collects image file paths from the root directory, limited to `image_num` images.\n",
    "        self.image_paths = sorted(\n",
    "            [os.path.join(root_dir, fname) for fname in os.listdir(root_dir)\n",
    "             if fname.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        )[:image_num]  # Limit to the first `image_num` images\n",
    "        self.transform = transform # Transformation to apply to images\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the number of images in the dataset.\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Loads an image by index, converts it to RGB, and applies transformations if provided.\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Since no actual labels, return 0 as dummy labels\n",
    "        return image, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2004425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation pipeline for data augmentation and normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    # Randomly applies a horizontal flip with 40% probability.\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ], p=0.4),  \n",
    "    transforms.ToTensor(), # Converts image to a PyTorch tensor.\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                         std=(0.5, 0.5, 0.5)), # Normalizes using mean and std\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654e691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(root_dir=\"/datasets/delkon/dm_data\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76ecb740",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f861f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2lightrainer.UnsupervisedLearning.DDPM.trainer_config import DDPMTrainerConfig\n",
    "from d2lightrainer.UnsupervisedLearning.DDPM.trainer import DDPMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20064da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm_cfg = DDPMTrainerConfig()\n",
    "new_param_dict = {\"device\": 3, \"save_dir\": \"runs_dm\", \"batch_size\": 16, \"optimizer\": \"AdamW\",\n",
    "                  \"validation_epochs\": 200, \"nominal_batch_size\": 64, \"img_size\": 64, \"epochs\": 600, \n",
    "                  \"warmup_epochs\": 20}\n",
    "ddpm_cfg.update(**new_param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm_trainer = DDPMTrainer(ddpm, dataset, ddpm_cfg)\n",
    "ddpm_trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
